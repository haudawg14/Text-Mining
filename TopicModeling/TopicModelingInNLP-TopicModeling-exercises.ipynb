{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc1c444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:37.384854Z",
     "iopub.status.busy": "2024-04-10T07:54:37.381695Z",
     "iopub.status.idle": "2024-04-10T07:54:37.406168Z",
     "shell.execute_reply": "2024-04-10T07:54:37.403052Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Topic Modeling In N L P: Topic Modeling : EXERCISES  ##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358dee4",
   "metadata": {},
   "source": [
    "#### Exercise ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481c101",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of TopicModelingInNLP - TopicModeling for Tasks 1-5\n",
    "#### Task 1:\n",
    "##### Add the packages needed for creating and working with an LDA model.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe42bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd41497",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "##### Use `Path` module from `pathlib` to point `data_dir` to your data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4155e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a112d47",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "##### Read in the \"UN_agreement_titles.csv\" dataset and save the dataframe as `ex_df`.\n",
    "##### Remove all rows from the dataset where `title` is empty. \n",
    "##### Subset the `title` column from `ex_df` and call it `ex_df_text`.\n",
    "##### Tokenize all documents in `ex_df_text` into a list of tokenized documents and save it as `ex_df_tokenized`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb08a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afaf81b9",
   "metadata": {},
   "source": [
    "#### Task 4:\n",
    "##### For each tokenized document in `ex_df_tokenized`, do the following:\n",
    "##### Convert the tokens to lowercase.\n",
    "##### Get common English stopwords from `nltk.corpus` and remove them from tokenized document.\n",
    "##### Remove punctuation and all non-alphabetical characters.\n",
    "##### Perform stemming of tokens using `PorterStemmer()`.\n",
    "##### Save the number of words in each document as a list and call it `ex_word_counts_per_document`.\n",
    "##### Save the list of clean documents as `ex_df_clean`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc518a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6064f60",
   "metadata": {},
   "source": [
    "#### Task 5:\n",
    "##### Convert word counts list and documents list to NumPy arrays and call them `ex_word_counts_array` and `ex_df_array` respectively.\n",
    "##### Filter out all documents containing less than 4 words and save the filtered array as a list with the name `ex_df_clean`. Check how many valid documents are left.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb816f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79e806d9",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of TopicModelingInNLP - TopicModeling for Tasks 6-9\n",
    "#### Task 6:\n",
    "##### Given the list of processed documents `ex_df_clean`, set a seed for reproducibility, create a `gensim` dictionary of corpus `ex_df_clean` and save it as `ex_dictionary`.\n",
    "##### Filter out from this dictionary words that occur in less than \"5\" documents and more than \"0.5\" documents. Remember that \"0.5\" is a fraction of the total corpus size.\n",
    "##### Make sure to keep the first \"200\" most frequent words.\n",
    "##### How many words are left in the dictionary?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e009905",
   "metadata": {},
   "source": [
    "#### Task 7:\n",
    "##### Use the dictionary created in Task 6 to transform each document in `ex_df_clean` into  bag-of-words.\n",
    "##### Build the `models.TfidfModel` transformation using the bag-of-words.\n",
    "##### Apply this transformation to the entire corpus.\n",
    "##### Inspect the TF-IDF scores for the first document.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b71f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ab4959",
   "metadata": {},
   "source": [
    "#### Task 8:\n",
    "##### Use the clean corpus `ex_df_clean`, dictionary `ex_dictionary` and TF-IDF matrix for the corpus `ex_corpus_tfidf`, build a parallelized LDA model to detect 3 topics.\n",
    "##### Use 3 workers and 2 passes for the LDA model. Save the model as `ex_lda_model_tfidf`.\n",
    "##### Print all topics and the top words within them.\n",
    "##### Classify the 2nd document as one of the 3 topics.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebbbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c6d10b",
   "metadata": {},
   "source": [
    "#### Task 9:\n",
    "##### Use the convenience function `compute_coherence_values()` from TopicModeling-3 slides to determine the best number of topics within the corpus. Remember to set a seed for reproducibility of your results.\n",
    "##### Plot a graph of number of topics and their coherence scores. Label the X-axis as \"Num Topics\" and Y-axis as \"Coherence Score\".\n",
    "##### What is the optimal number of topics according to you?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2d441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe6efc0e",
   "metadata": {},
   "source": [
    "#### Please refer to module 4 of TopicModelingInNLP - TopicModeling for Tasks 10-12\n",
    "#### Task 10:\n",
    "##### Using `pyLDAvis` package, prepare LDA vis object of the LDA model `ex_lda_model_tfidf` that you created in Task 8. Call this object as `ex_vis`.\n",
    "##### Display the `ex_vis` object.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1753484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "105724f6",
   "metadata": {},
   "source": [
    "#### Task 11:\n",
    "##### Loop through the corpus and find the best topic for each document. Hint: Use the convenience function `GetDocTopicPair` from TopicModeling-4 slides that takes in a document index, the corpus TF-IDF matrix and an LDA model to return a tuple of the document index, the best fit topic, and its probability.\n",
    "##### Convert the list of tuples into a dataframe and call it `ex_doc_topic_pairs_df`.\n",
    "##### Assign column names to `ex_doc_topic_pairs_df`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac80f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6fe263a",
   "metadata": {},
   "source": [
    "#### Task 12:\n",
    "##### Find the original indices of valid documents that we kept in Task 5. Set them as the index of their respective documents in `ex_doc_topic_pairs_df`.\n",
    "##### Find all the documents assigned to topic 1 (index = 0).\n",
    "##### Sort the documents in decreasing order of probability.\n",
    "##### Save the pyLDA vis object as an HTML file.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e543b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
