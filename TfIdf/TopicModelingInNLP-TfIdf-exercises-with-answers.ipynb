{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640feeb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:45.961896Z",
     "iopub.status.busy": "2024-04-10T07:54:45.958848Z",
     "iopub.status.idle": "2024-04-10T07:54:45.983507Z",
     "shell.execute_reply": "2024-04-10T07:54:45.980471Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Topic Modeling In N L P: Tf Idf : EXERCISES ANSWERS ##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed5ab7",
   "metadata": {},
   "source": [
    "#### Exercise ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd2b82",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of TopicModelingInNLP - TfIdf for Tasks 1-8\n",
    "#### Task 1:\n",
    "##### Add the packages needed for creating and working with TF-IDF.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bccd8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:45.999315Z",
     "iopub.status.busy": "2024-04-10T07:54:45.997538Z",
     "iopub.status.idle": "2024-04-10T07:54:52.523328Z",
     "shell.execute_reply": "2024-04-10T07:54:52.520169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f79b33",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "##### Use `Path` module from `pathlib` to point `data_dir` to your data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8c1ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:52.539444Z",
     "iopub.status.busy": "2024-04-10T07:54:52.538069Z",
     "iopub.status.idle": "2024-04-10T07:54:52.552725Z",
     "shell.execute_reply": "2024-04-10T07:54:52.549944Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path(\".\").resolve()\n",
    "# Set `main_dir` to the location of your course folder.\n",
    "main_dir = home_dir.parent.parent\n",
    "# Make `data_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9bcd0",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "##### Read in the \"UN_agreement_titles.csv\" dataset and save the dataframe as `ex_df`.\n",
    "##### Remove all rows from the dataset where `title` is empty. \n",
    "##### Subset the `title` column from `ex_df` and call it `ex_df_text`.\n",
    "##### Tokenize all documents in `ex_df_text` into a list of tokenized documents and save it as `ex_df_tokenized`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f6f0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:52.563994Z",
     "iopub.status.busy": "2024-04-10T07:54:52.563090Z",
     "iopub.status.idle": "2024-04-10T07:54:53.451778Z",
     "shell.execute_reply": "2024-04-10T07:54:53.449077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Agreement', 'concerning', 'arrangements', 'for', 'the', 'holding', 'of', 'the', 'fifth', 'session', 'of', 'the', 'Council', 'of', 'the', 'United', 'Nations', 'Relief', 'and', 'Rehabilitation', 'Administration', 'at', 'the', 'Palais', 'des', 'Nations', ',', 'Geneva', ',', 'concluded', 'between', 'the', 'United', 'Nations', ',', 'the', 'League', 'of', 'Nations', 'and', 'the', 'United', 'Nations', 'Relief', 'and', 'Rehabilitation', 'Administration'], ['Declaration', 'recognizing', 'as', 'compulsory', 'the', 'jurisdiction', 'of', 'the', 'Court', ',', 'in', 'conformity', 'with', 'Article', '36', ',', 'paragraph', '2', ',', 'of', 'the', 'Statute', 'of', 'the', 'International', 'Court', 'of', 'Justice', ',', 'in', 'all', 'legal', 'disputes', 'concerning', 'the', 'interpretation', ',', 'application', 'or', 'validity', 'of', 'any', 'treaty', 'relating', 'to', 'the', 'boundaries', 'of', 'British', 'Honduras', '.', 'London', ',', '13', 'February', '1946'], ['Declaration', 'renewing', 'for', 'a', 'further', 'period', 'of', 'five', 'years', ',', 'beginning', 'on', '12', 'February', '1951', ',', 'the', 'Declaration', 'recognizing', 'as', 'compulsory', 'the', 'jurisdiction', 'of', 'the', 'Court', ',', 'in', 'conformity', 'with', 'Article', '36', ',', 'paragraph', '2', ',', 'of', 'the', 'Statute', 'of', 'the', 'International', 'Court', 'of', 'Justice', ',', 'in', 'all', 'legal', 'disputes', 'concerning', 'the', 'interpretation', ',', 'application', 'or', 'validity', 'of', 'any', 'treaty', 'relating', 'to', 'the', 'boundaries', 'of', 'British', 'Honduras', '.', 'London', ',', '13', 'February', '1946'], ['Declaration', 'recognizing', 'as', 'compulsory', 'the', 'jurisdiction', 'of', 'the', 'Court', ',', 'in', 'conformity', 'with', 'Article', '36', ',', 'paragraph', '2', ',', 'of', 'the', 'Statute', 'of', 'the', 'International', 'Court', 'of', 'Justice', '.', 'New', 'York', ',', '10', 'December', '1946'], ['Convention', 'between', 'the', 'United', 'Kingdom', 'and', 'France', 'respecting', 'commercial', 'relations', 'between', 'France', 'and', 'the', 'Seychelles', 'Islands', ',', 'signed', 'at', 'London', ',', 'April', '16', ',', '1902', '.', 'Denunciation', 'on', 'the', 'part', 'of', 'the', 'United', 'Kingdom']]\n"
     ]
    }
   ],
   "source": [
    "ex_df = pd.read_csv(str(data_dir)+\"/\"+ \"UN_agreement_titles.csv\")\n",
    "# Drop NAs if any.\n",
    "ex_df = ex_df.dropna(subset = [\"title\"]).reset_index(drop=True)\n",
    "\n",
    "# Isolate the `snippet` column.\n",
    "ex_df_text = ex_df[\"title\"]\n",
    "\n",
    "# Tokenize each document into a large list of tokenized documents.\n",
    "ex_df_tokenized = [word_tokenize(ex_df_text[i]) for i in range(0, len(ex_df_text))]\n",
    "print(ex_df_tokenized[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be036d",
   "metadata": {},
   "source": [
    "#### Task 4:\n",
    "##### For each tokenized document in `ex_df_tokenized`, do the following:\n",
    "##### Convert the tokens to lowercase.\n",
    "##### Get common English stopwords from `nltk.corpus` and remove them from tokenized document.\n",
    "##### Remove punctuation and all non-alphabetical characters.\n",
    "##### Perform stemming of tokens using `PorterStemmer()`.\n",
    "##### Save the number of words in each document as a list and call it `ex_word_counts_per_document`.\n",
    "##### Save the list of clean documents as `ex_df_clean`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe3b84a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:53.462329Z",
     "iopub.status.busy": "2024-04-10T07:54:53.461406Z",
     "iopub.status.idle": "2024-04-10T07:54:55.010748Z",
     "shell.execute_reply": "2024-04-10T07:54:55.008309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get common English stop words.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Create a list for clean documents.\n",
    "ex_df_clean = [None] * len(ex_df_tokenized)\n",
    "\n",
    "# Create a list of word counts for each clean document.\n",
    "ex_word_counts_per_document = [None] * len(ex_df_tokenized)\n",
    "\n",
    "# Process words in all documents.\n",
    "for i in range(len(ex_df_tokenized)):\n",
    "    # 1. Convert to lowercase.\n",
    "    ex_df_clean[i] = [document.lower() for document in ex_df_tokenized[i]]\n",
    "    # 2. Remove stop words.\n",
    "    ex_df_clean[i] = [word for word in ex_df_clean[i] if not word in stop_words]\n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    ex_df_clean[i] = [word for word in ex_df_clean[i] if word.isalpha()]\n",
    "    # 4. Stem words.\n",
    "    ex_df_clean[i] = [PorterStemmer().stem(word) for word in ex_df_clean[i]]\n",
    "    # Record the word count per document.\n",
    "    ex_word_counts_per_document[i] = len(ex_df_clean[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1aa7f8",
   "metadata": {},
   "source": [
    "#### Task 5:\n",
    "##### Convert word counts list and documents list to NumPy arrays and call them `ex_word_counts_array` and `ex_df_array` respectively.\n",
    "##### Filter out all documents containing less than 4 words and save the filtered array as a list with the name `ex_df_clean`. Check how many valid documents are left.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bfc109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:55.021901Z",
     "iopub.status.busy": "2024-04-10T07:54:55.020963Z",
     "iopub.status.idle": "2024-04-10T07:54:55.041815Z",
     "shell.execute_reply": "2024-04-10T07:54:55.039336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n",
      "857\n",
      "[['agreement', 'concern', 'arrang', 'hold', 'fifth', 'session', 'council', 'unit', 'nation', 'relief', 'rehabilit', 'administr', 'palai', 'de', 'nation', 'geneva', 'conclud', 'unit', 'nation', 'leagu', 'nation', 'unit', 'nation', 'relief', 'rehabilit', 'administr'], ['declar', 'recogn', 'compulsori', 'jurisdict', 'court', 'conform', 'articl', 'paragraph', 'statut', 'intern', 'court', 'justic', 'legal', 'disput', 'concern', 'interpret', 'applic', 'valid', 'treati', 'relat', 'boundari', 'british', 'hondura', 'london', 'februari'], ['declar', 'renew', 'period', 'five', 'year', 'begin', 'februari', 'declar', 'recogn', 'compulsori', 'jurisdict', 'court', 'conform', 'articl', 'paragraph', 'statut', 'intern', 'court', 'justic', 'legal', 'disput', 'concern', 'interpret', 'applic', 'valid', 'treati', 'relat', 'boundari', 'british', 'hondura', 'london', 'februari']]\n"
     ]
    }
   ],
   "source": [
    "ex_word_counts_array = np.array(ex_word_counts_per_document)\n",
    "ex_df_array = np.array(ex_df_clean, dtype= object)\n",
    "print(len(ex_df_array))\n",
    "\n",
    "ex_valid_documents = np.where(ex_word_counts_array >= 4)[0]\n",
    "ex_df_array = ex_df_array[ex_valid_documents]\n",
    "print(len(ex_df_array))\n",
    "\n",
    "# Convert the array back to a list.\n",
    "ex_df_clean = ex_df_array.tolist()\n",
    "print(ex_df_clean[0:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44184569",
   "metadata": {},
   "source": [
    "#### Task 6:\n",
    "##### Given the list of processed documents `ex_df_clean`, set a seed for reproducibility, create a `gensim` dictionary of corpus `ex_df_clean` and save it as `ex_dictionary`.\n",
    "##### Filter out from this dictionary words that occur in less than \"5\" documents and more than \"0.5\" documents. Remember that \"0.5\" is a fraction of the total corpus size.\n",
    "##### Make sure to keep the first \"200\" most frequent words.\n",
    "##### How many words are left in the dictionary?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7487f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:55.051457Z",
     "iopub.status.busy": "2024-04-10T07:54:55.050602Z",
     "iopub.status.idle": "2024-04-10T07:54:55.146325Z",
     "shell.execute_reply": "2024-04-10T07:54:55.143851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed.\n",
    "np.random.seed(100)\n",
    "ex_dictionary = gensim.corpora.Dictionary(ex_df_clean)\n",
    "\n",
    "ex_dictionary.filter_extremes(no_below = 5, no_above = 0.5, keep_n = 200)\n",
    "\n",
    " # Words left in the dictionary.\n",
    "len(ex_dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e352ed",
   "metadata": {},
   "source": [
    "#### Task 7:\n",
    "##### Use the dictionary created in Task 2 to transform each document in `ex_df_clean` into  bag-of-words.\n",
    "##### Build the `models.TfidfModel` transformation using the bag-of-words.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e731141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:55.156020Z",
     "iopub.status.busy": "2024-04-10T07:54:55.155141Z",
     "iopub.status.idle": "2024-04-10T07:54:55.211033Z",
     "shell.execute_reply": "2024-04-10T07:54:55.208734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 1), (3, 1), (4, 5), (5, 2), (6, 3)]\n"
     ]
    }
   ],
   "source": [
    "ex_bow_corpus = [ex_dictionary.doc2bow(doc) for doc in ex_df_clean]\n",
    "print(ex_bow_corpus[0])\n",
    "\n",
    "# Define the transformation.\n",
    "ex_tfidf = models.TfidfModel(ex_bow_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab45325",
   "metadata": {},
   "source": [
    "#### Task 8:\n",
    "##### Apply this transformation to the entire corpus.\n",
    "##### Inspect the TF-IDF scores for the first document.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e7c2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:54:55.220460Z",
     "iopub.status.busy": "2024-04-10T07:54:55.219591Z",
     "iopub.status.idle": "2024-04-10T07:54:55.234764Z",
     "shell.execute_reply": "2024-04-10T07:54:55.232431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.37421377520832816),\n",
      " (1, 0.18710688760416408),\n",
      " (2, 0.06475646625501938),\n",
      " (3, 0.1593591956618582),\n",
      " (4, 0.7846615184322249),\n",
      " (5, 0.36478372202169385),\n",
      " (6, 0.21588294868103092)]\n"
     ]
    }
   ],
   "source": [
    "# Apply the transformation\n",
    "ex_corpus_tfidf = ex_tfidf[ex_bow_corpus]\n",
    "\n",
    "# Preview TF-IDF scores for the first document.\n",
    "for doc in ex_corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
