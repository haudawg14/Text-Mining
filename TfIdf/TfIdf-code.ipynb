{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1c66c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:04.430067Z",
     "iopub.status.busy": "2024-04-10T07:55:04.426945Z",
     "iopub.status.idle": "2024-04-10T07:55:04.452129Z",
     "shell.execute_reply": "2024-04-10T07:55:04.449232Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Topic Modeling In N L P: Tf Idf - 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80aa62a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:04.465850Z",
     "iopub.status.busy": "2024-04-10T07:55:04.464284Z",
     "iopub.status.idle": "2024-04-10T07:55:11.134037Z",
     "shell.execute_reply": "2024-04-10T07:55:11.131593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 13/43: Load packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9776c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.147684Z",
     "iopub.status.busy": "2024-04-10T07:55:11.146293Z",
     "iopub.status.idle": "2024-04-10T07:55:11.163609Z",
     "shell.execute_reply": "2024-04-10T07:55:11.161064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/app/20240410_070845231715/assets\n",
      "/usr/app/20240410_070845231715/assets/data\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 14/43: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250393d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.174127Z",
     "iopub.status.busy": "2024-04-10T07:55:11.172720Z",
     "iopub.status.idle": "2024-04-10T07:55:11.243671Z",
     "shell.execute_reply": "2024-04-10T07:55:11.241274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             web_url  \\\n",
      "0  https://www.nytimes.com/reuters/2019/01/01/spo...   \n",
      "1  https://www.nytimes.com/reuters/2019/01/01/wor...   \n",
      "2  https://www.nytimes.com/aponline/2019/01/01/sp...   \n",
      "3  https://www.nytimes.com/2019/01/09/arts/design...   \n",
      "4  https://www.nytimes.com/aponline/2019/01/10/sp...   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Kyrgios, Murray Power Into Second Round in Bri...   \n",
      "1  UK Police Treating Manchester Stabbing Attack ...   \n",
      "2  Former NFL Player Wiley Talks Playoffs on Podc...   \n",
      "3    After the Quake, Dana Schutz Gets Back to Work    \n",
      "4  Ogunbowale Helps Irish Beat Cardinals in 1-2 S...   \n",
      "\n",
      "                                             snippet  word_count  \\\n",
      "0  Nick Kyrgios started his Brisbane Open title d...         435   \n",
      "1  British police confirmed on Tuesday they were ...          81   \n",
      "2  Marcellus Wiley is still on the fence about le...         272   \n",
      "3  Still reckoning with the fallout from her Emme...        1540   \n",
      "4  As far as Arike Ogunbowale and coach Muffet Mc...        1059   \n",
      "\n",
      "               source type_of_material        date  id  \n",
      "0             Reuters             News  2019-01-01   8  \n",
      "1             Reuters             News  2019-01-01   9  \n",
      "2                  AP             News  2019-01-01  10  \n",
      "3  The New York Times             News  2019-01-09  11  \n",
      "4                  AP             News  2019-01-11  12  \n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 17/43: Load data  ####\n",
    "\n",
    "# Let's load and prepare the dataset for creating Document-Term Matrix\n",
    "df = pd.read_csv(str(data_dir)+\"/\"+ \"NYT_article_data.csv\")\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea17a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.254502Z",
     "iopub.status.busy": "2024-04-10T07:55:11.253042Z",
     "iopub.status.idle": "2024-04-10T07:55:11.285059Z",
     "shell.execute_reply": "2024-04-10T07:55:11.282639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 18/43: Check for NAs  ####\n",
    "\n",
    "# Print total number of NAs.\n",
    "print(df[\"snippet\"].isna().sum())\n",
    "# Drop NAs if any.\n",
    "df = df.dropna(subset = [\"snippet\"]).reset_index(drop=True)\n",
    "print(df[\"snippet\"].isna().sum())\n",
    "# Isolate the `snippet` column.\n",
    "df_text = df[\"snippet\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2d6645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.294634Z",
     "iopub.status.busy": "2024-04-10T07:55:11.293739Z",
     "iopub.status.idle": "2024-04-10T07:55:11.558959Z",
     "shell.execute_reply": "2024-04-10T07:55:11.556894Z"
    }
   },
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19/43: Tokenization: split each document into words  ####\n",
    "\n",
    "# Tokenize each document into a large list of tokenized documents.\n",
    "df_tokenized = [word_tokenize(df_text[i]) for i in range(0, len(df_text))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81fa592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.569980Z",
     "iopub.status.busy": "2024-04-10T07:55:11.568611Z",
     "iopub.status.idle": "2024-04-10T07:55:11.583491Z",
     "shell.execute_reply": "2024-04-10T07:55:11.581214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'Kyrgios', 'started', 'his', 'Brisbane', 'Open', 'title', 'defense', 'with', 'a', 'battling', '7-6', '(', '5', ')', '5-7', '7-6', '(', '5', ')', 'victory', 'over', 'American', 'Ryan', 'Harrison', 'in', 'the', 'opening', 'round', 'on', 'Tuesday', '.']\n",
      "['nick', 'kyrgios', 'started', 'his', 'brisbane', 'open', 'title', 'defense', 'with', 'a']\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 21/43: Convert characters to lowercase  ####\n",
    "\n",
    "# Let's take a look at the first tokenized document\n",
    "document_words = df_tokenized[0]\n",
    "print(document_words)\n",
    "# 1. Convert to lowercase.\n",
    "document_words = [word.lower() for word in document_words]\n",
    "print(document_words[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bdb4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.594316Z",
     "iopub.status.busy": "2024-04-10T07:55:11.592904Z",
     "iopub.status.idle": "2024-04-10T07:55:11.614026Z",
     "shell.execute_reply": "2024-04-10T07:55:11.611773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['nick', 'kyrgios', 'started', 'brisbane', 'open', 'title', 'defense', 'battling', '7-6', '(']\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 22/43: Remove stop words  ####\n",
    "\n",
    "# 2. Remove stop words.\n",
    "# Get common English stop words.\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:10])\n",
    "# Remove stop words.\n",
    "document_words = [word for word in document_words if not word in stop_words]\n",
    "print(document_words[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3507d9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.623756Z",
     "iopub.status.busy": "2024-04-10T07:55:11.622891Z",
     "iopub.status.idle": "2024-04-10T07:55:11.636083Z",
     "shell.execute_reply": "2024-04-10T07:55:11.633805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nick', 'kyrgios', 'started', 'brisbane', 'open', 'title', 'defense', 'battling', 'victory', 'american']\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 23/43: Remove non-alphabetical characters  ####\n",
    "\n",
    "# 3. Remove punctuation and any non-alphabetical characters.\n",
    "document_words = [word for word in document_words if word.isalpha()]\n",
    "print(document_words[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeed1c0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.645997Z",
     "iopub.status.busy": "2024-04-10T07:55:11.645139Z",
     "iopub.status.idle": "2024-04-10T07:55:11.660410Z",
     "shell.execute_reply": "2024-04-10T07:55:11.658145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nick', 'kyrgio', 'start', 'brisban', 'open', 'titl', 'defens', 'battl', 'victori', 'american']\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 26/43: Stem words  ####\n",
    "\n",
    "# 4. Stem words.\n",
    "document_words = [PorterStemmer().stem(word) for word in document_words]\n",
    "print(document_words[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef0dff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:11.670680Z",
     "iopub.status.busy": "2024-04-10T07:55:11.669818Z",
     "iopub.status.idle": "2024-04-10T07:55:12.276376Z",
     "shell.execute_reply": "2024-04-10T07:55:12.274015Z"
    }
   },
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 27/43: Clean the entire corpus  ####\n",
    "\n",
    "# Create a list for clean documents.\n",
    "df_clean = [None] * len(df_tokenized)\n",
    "# Create a list of word counts for each clean document.\n",
    "word_counts_per_document = [None] * len(df_tokenized)\n",
    "\n",
    "# Process words in all documents.\n",
    "for i in range(len(df_tokenized)):\n",
    "    # 1. Convert to lowercase.\n",
    "    df_clean[i] = [document.lower() for document in df_tokenized[i]]\n",
    "    \n",
    "    # 2. Remove stop words.\n",
    "    df_clean[i] = [word for word in df_clean[i] if not word in stop_words]\n",
    "    \n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    df_clean[i] = [word for word in df_clean[i] if word.isalpha()]\n",
    "    \n",
    "    # 4. Stem words.\n",
    "    df_clean[i] = [PorterStemmer().stem(word) for word in df_clean[i]]\n",
    "    \n",
    "    # Record the word count per document.\n",
    "    word_counts_per_document[i] = len(df_clean[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a705b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.288763Z",
     "iopub.status.busy": "2024-04-10T07:55:12.287853Z",
     "iopub.status.idle": "2024-04-10T07:55:12.303565Z",
     "shell.execute_reply": "2024-04-10T07:55:12.301213Z"
    }
   },
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 28/43: Clean the entire corpus (cont'd)  ####\n",
    "\n",
    "# Convert word counts list and documents list to NumPy arrays.\n",
    "word_counts_array = np.array(word_counts_per_document)\n",
    "df_array = np.array(df_clean, dtype= object)\n",
    "\n",
    "# Find indices of all documents where there are greater than or equal to 5 words.\n",
    "valid_documents = np.where(word_counts_array >= 5)[0]\n",
    "\n",
    "# Subset the df_array to keep only those where there are at least 5 words.\n",
    "df_array = df_array[valid_documents]\n",
    "\n",
    "# Convert the array back to a list.\n",
    "df_clean = df_array.tolist()   #<- the processed documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74f12f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.312871Z",
     "iopub.status.busy": "2024-04-10T07:55:12.312002Z",
     "iopub.status.idle": "2024-04-10T07:55:12.323597Z",
     "shell.execute_reply": "2024-04-10T07:55:12.321307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nick', 'kyrgio', 'start', 'brisban', 'open', 'titl', 'defens', 'battl', 'victori', 'american', 'ryan', 'harrison', 'open', 'round', 'tuesday'], ['british', 'polic', 'confirm', 'tuesday', 'treat', 'stab', 'attack', 'injur', 'three', 'peopl', 'manchest', 'victoria', 'train', 'station', 'terrorist', 'investig', 'search', 'address', 'cheetham', 'hill', 'area', 'citi']]\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 30/43: Data for TF-IDF matrix   ####\n",
    "\n",
    "print(df_clean[0:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b1d056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.343963Z",
     "iopub.status.busy": "2024-04-10T07:55:12.342499Z",
     "iopub.status.idle": "2024-04-10T07:55:12.388734Z",
     "shell.execute_reply": "2024-04-10T07:55:12.386436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 american\n",
      "1 battl\n",
      "2 brisban\n",
      "3 defens\n",
      "4 harrison\n",
      "5 kyrgio\n",
      "6 nick\n",
      "7 open\n",
      "8 round\n",
      "9 ryan\n",
      "10 start\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 32/43: Create a dictionary of counts   ####\n",
    "\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "dictionary = gensim.corpora.Dictionary(df_clean)\n",
    "\n",
    "# The loop below iterates through the first 10 items of the dictionary and prints out the key and value. \n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2a717f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.398439Z",
     "iopub.status.busy": "2024-04-10T07:55:12.397554Z",
     "iopub.status.idle": "2024-04-10T07:55:12.433248Z",
     "shell.execute_reply": "2024-04-10T07:55:12.430776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 33/43: Create a dictionary of counts (cont'd)  ####\n",
    "\n",
    "dictionary.filter_extremes(no_below = 4, no_above = 0.5, keep_n = 200)\n",
    "\n",
    "# How many words are left in the dictionary?\n",
    "len(dictionary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbb64f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.444078Z",
     "iopub.status.busy": "2024-04-10T07:55:12.443016Z",
     "iopub.status.idle": "2024-04-10T07:55:12.466614Z",
     "shell.execute_reply": "2024-04-10T07:55:12.464388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 34/43: Document to bag-of-words  ####\n",
    "\n",
    "# We use a list comprehension to transform each doc within our df_clean object.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in df_clean]\n",
    "\n",
    "# Let's look at the first document.\n",
    "print(bow_corpus[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca43f46a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.476958Z",
     "iopub.status.busy": "2024-04-10T07:55:12.476084Z",
     "iopub.status.idle": "2024-04-10T07:55:12.491872Z",
     "shell.execute_reply": "2024-04-10T07:55:12.489618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"american\") appears 1 time.\n",
      "Word 1 (\"defens\") appears 1 time.\n",
      "Word 2 (\"open\") appears 2 time.\n",
      "Word 3 (\"round\") appears 1 time.\n",
      "Word 4 (\"start\") appears 1 time.\n",
      "Word 5 (\"tuesday\") appears 1 time.\n",
      "Word 6 (\"victori\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 35/43: Document to bag-of-words  ####\n",
    "\n",
    "# Isolate the first document.\n",
    "bow_doc_1 = bow_corpus[0]\n",
    "\n",
    "# Iterate through each dictionary item using the index. \n",
    "# Print out each actual word and how many times it appears.\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0],\n",
    "          \n",
    "          dictionary[bow_doc_1[i][0]],\n",
    "          bow_doc_1[i][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ab88c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.501848Z",
     "iopub.status.busy": "2024-04-10T07:55:12.500965Z",
     "iopub.status.idle": "2024-04-10T07:55:12.521664Z",
     "shell.execute_reply": "2024-04-10T07:55:12.519364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.31942373876087665),\n",
      " (1, 0.3549009519669791),\n",
      " (2, 0.6118718565633235),\n",
      " (3, 0.3549009519669791),\n",
      " (4, 0.3059359282816618),\n",
      " (5, 0.22829905152454918),\n",
      " (6, 0.3549009519669791)]\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 37/43: Transform counts with TfidfModel  ####\n",
    "\n",
    "# This is the transformation.\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply the transformation to the entire corpus.\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# Preview TF-IDF scores for the first document.\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6119a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:12.531179Z",
     "iopub.status.busy": "2024-04-10T07:55:12.530286Z",
     "iopub.status.idle": "2024-04-10T07:55:12.539702Z",
     "shell.execute_reply": "2024-04-10T07:55:12.537467Z"
    }
   },
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 40/43: Exercise  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
