{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640feeb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:55:21.677108Z",
     "iopub.status.busy": "2024-04-10T07:55:21.674453Z",
     "iopub.status.idle": "2024-04-10T07:55:21.698284Z",
     "shell.execute_reply": "2024-04-10T07:55:21.695172Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Topic Modeling In N L P: Tf Idf : EXERCISES  ##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed5ab7",
   "metadata": {},
   "source": [
    "#### Exercise ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd2b82",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of TopicModelingInNLP - TfIdf for Tasks 1-8\n",
    "#### Task 1:\n",
    "##### Add the packages needed for creating and working with TF-IDF.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bccd8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28f79b33",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "##### Use `Path` module from `pathlib` to point `data_dir` to your data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c1ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e9bcd0",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "##### Read in the \"UN_agreement_titles.csv\" dataset and save the dataframe as `ex_df`.\n",
    "##### Remove all rows from the dataset where `title` is empty. \n",
    "##### Subset the `title` column from `ex_df` and call it `ex_df_text`.\n",
    "##### Tokenize all documents in `ex_df_text` into a list of tokenized documents and save it as `ex_df_tokenized`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5be036d",
   "metadata": {},
   "source": [
    "#### Task 4:\n",
    "##### For each tokenized document in `ex_df_tokenized`, do the following:\n",
    "##### Convert the tokens to lowercase.\n",
    "##### Get common English stopwords from `nltk.corpus` and remove them from tokenized document.\n",
    "##### Remove punctuation and all non-alphabetical characters.\n",
    "##### Perform stemming of tokens using `PorterStemmer()`.\n",
    "##### Save the number of words in each document as a list and call it `ex_word_counts_per_document`.\n",
    "##### Save the list of clean documents as `ex_df_clean`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3b84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e1aa7f8",
   "metadata": {},
   "source": [
    "#### Task 5:\n",
    "##### Convert word counts list and documents list to NumPy arrays and call them `ex_word_counts_array` and `ex_df_array` respectively.\n",
    "##### Filter out all documents containing less than 4 words and save the filtered array as a list with the name `ex_df_clean`. Check how many valid documents are left.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfc109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44184569",
   "metadata": {},
   "source": [
    "#### Task 6:\n",
    "##### Given the list of processed documents `ex_df_clean`, set a seed for reproducibility, create a `gensim` dictionary of corpus `ex_df_clean` and save it as `ex_dictionary`.\n",
    "##### Filter out from this dictionary words that occur in less than \"5\" documents and more than \"0.5\" documents. Remember that \"0.5\" is a fraction of the total corpus size.\n",
    "##### Make sure to keep the first \"200\" most frequent words.\n",
    "##### How many words are left in the dictionary?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7487f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e352ed",
   "metadata": {},
   "source": [
    "#### Task 7:\n",
    "##### Use the dictionary created in Task 2 to transform each document in `ex_df_clean` into  bag-of-words.\n",
    "##### Build the `models.TfidfModel` transformation using the bag-of-words.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e731141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab45325",
   "metadata": {},
   "source": [
    "#### Task 8:\n",
    "##### Apply this transformation to the entire corpus.\n",
    "##### Inspect the TF-IDF scores for the first document.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7c2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
